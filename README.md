# learning-curve-sample-complexity
# Learning Curve & Sample Complexity Analysis

This project demonstrates how model performance changes as the amount of training data increases.  
It uses a Decision Tree classifier on a synthetic binary classification dataset to empirically study learning curves and sample complexity.

---

## ğŸ“Œ Objective
- Understand how accuracy varies with training set size
- Visualize learning behavior of a bounded-complexity model
- Study generalization and data sufficiency

---

## ğŸ§  Key Concepts Covered
- Trainâ€“test split
- Decision Tree classifier
- Model generalization
- Learning curve
- Sample complexity
- Biasâ€“variance tradeoff (empirical)

---

## ğŸ› ï¸ Tech Stack
- Python
- NumPy
- pandas
- scikit-learn
- Matplotlib
- Jupyter Notebook

---

## ğŸ“Š Methodology
1. Generate synthetic 2D classification data
2. Train a Decision Tree with fixed max depth
3. Vary training data size from 10% to 90%
4. Evaluate accuracy on unseen test data
5. Plot training size vs accuracy (learning curve)

---

## ğŸ“ˆ Result
The learning curve shows how accuracy improves with more data and eventually plateaus, indicating the modelâ€™s capacity limit.

---

## ğŸ“‚ Files
- `learning_curve_sample_complexity.ipynb` â€” main notebook with code and explanations

---

## ğŸš€ How to Run
```bash
pip install numpy pandas matplotlib scikit-learn
